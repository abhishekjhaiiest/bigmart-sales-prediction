{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91dxjAWM8Krw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# --- 1. Data Preparation ---\n",
        "# Load and combine data\n",
        "train = pd.read_csv('train_v9rqX0R.csv')\n",
        "test = pd.read_csv('test_AbJTz2l.csv')\n",
        "data = pd.concat([train.assign(source='train'), test.assign(source='test')], ignore_index=True)\n",
        "\n",
        "# Impute Item_Weight\n",
        "item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
        "overall_avg_weight = data['Item_Weight'].mean()\n",
        "def get_item_weight(item_id):\n",
        "    if item_id in item_avg_weight.index:\n",
        "        return item_avg_weight.loc[item_id, 'Item_Weight']\n",
        "    return overall_avg_weight\n",
        "data.loc[data['Item_Weight'].isnull(),'Item_Weight'] = data.loc[data['Item_Weight'].isnull(),'Item_Identifier'].apply(get_item_weight)\n",
        "\n",
        "# Impute Outlet_Size\n",
        "outlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x: x.mode()[0]) )\n",
        "data.loc[data['Outlet_Size'].isnull(),'Outlet_Size'] = data.loc[data['Outlet_Size'].isnull(),'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\n",
        "\n",
        "# Impute Item_Visibility 0s\n",
        "visibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
        "overall_avg_visibility = data['Item_Visibility'].mean()\n",
        "def get_avg_visibility(item_id):\n",
        "    if item_id in visibility_avg.index:\n",
        "        return visibility_avg.loc[item_id, 'Item_Visibility']\n",
        "    return overall_avg_visibility\n",
        "miss_bool = (data['Item_Visibility'] == 0)\n",
        "data.loc[miss_bool,'Item_Visibility'] = data.loc[miss_bool,'Item_Identifier'].apply(get_avg_visibility)\n",
        "\n",
        "# Feature Engineering\n",
        "data['Item_Visibility_MeanRatio'] = data.apply(lambda x: x['Item_Visibility']/get_avg_visibility(x['Item_Identifier']), axis=1)\n",
        "data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
        "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular', 'low fat':'Low Fat'})\n",
        "data['Item_Type_Combined'] = data['Item_Type'].apply(lambda x: 'Food' if x in ['Dairy', 'Soft Drinks', 'Meat', 'Fruits and Vegetables', 'Baking Goods', 'Snack Foods', 'Frozen Foods', 'Breakfast', 'Health and Hygiene', 'Hard Drinks', 'Canned', 'Breads', 'Starchy Foods'] else ('Non-Consumable' if x in ['Household', 'Others'] else 'Drinks'))\n",
        "data.loc[data['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\n",
        "\n",
        "# Label Encoding\n",
        "le = LabelEncoder()\n",
        "data['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\n",
        "var_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\n",
        "for i in var_mod:\n",
        "    data[i] = le.fit_transform(data[i])\n",
        "\n",
        "# One Hot Encoding\n",
        "data = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type_Combined','Outlet'])\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'bool':\n",
        "        data[col] = data[col].astype('float64')\n",
        "\n",
        "# Final split and clean\n",
        "data.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n",
        "train = data.loc[data['source']==\"train\"].copy()\n",
        "test = data.loc[data['source']==\"test\"].copy()\n",
        "test.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\n",
        "train.drop(['source'],axis=1,inplace=True)\n",
        "\n",
        "# Define features\n",
        "predictors = [x for x in train.columns if x not in ['Item_Outlet_Sales', 'Item_Identifier','Outlet_Identifier']]\n",
        "y = train['Item_Outlet_Sales']\n",
        "X = train[predictors]\n",
        "X_test = test[predictors]\n",
        "\n",
        "\n",
        "# --- 2. FINAL MODEL DEFINITIONS ---\n",
        "\n",
        "# 1. LGBM V3: Increased Complexity (from 64 to 128 leaves)\n",
        "best_lgb_v3 = LGBMRegressor(\n",
        "    subsample=0.8, reg_lambda=1, reg_alpha=0, num_leaves=128, n_estimators=3000,\n",
        "    min_child_samples=50, max_depth=12, learning_rate=0.005, colsample_bytree=0.8,\n",
        "    random_state=RANDOM_SEED, n_jobs=-1\n",
        ")\n",
        "\n",
        "# 2. XGBoost V3: Increased Complexity (from depth 8 to depth 9)\n",
        "best_xgb_reg_v3 = XGBRegressor(\n",
        "    subsample=0.8, reg_lambda=1.0, reg_alpha=0.1, n_estimators=2000,\n",
        "    max_depth=9, learning_rate=0.01, colsample_bytree=0.6,\n",
        "    random_state=RANDOM_SEED, n_jobs=-1, verbosity=0, tree_method=\"auto\"\n",
        ")\n",
        "\n",
        "# 3. CatBoost V2 (Kept stable, as it performed well)\n",
        "best_cat_v2 = CatBoostRegressor(\n",
        "    iterations=3500, learning_rate=0.02, depth=6, eval_metric='RMSE',\n",
        "    random_seed=RANDOM_SEED, early_stopping_rounds=100,\n",
        "    logging_level='Silent', allow_writing_files=False\n",
        ")\n",
        "\n",
        "# 4. Random Forest (Reverting to safer settings to reduce noise)\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=400, max_depth=6, min_samples_leaf=100,\n",
        "    n_jobs=-1, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "estimators_v3 = [\n",
        "    ('lgb', best_lgb_v3),\n",
        "    ('xgb', best_xgb_reg_v3),\n",
        "    ('cat', best_cat_v2),\n",
        "    ('rf', rf)\n",
        "]\n",
        "\n",
        "stack_reg_v3 = StackingRegressor(\n",
        "    estimators=estimators_v3,\n",
        "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]),\n",
        "    passthrough=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Training Final Stacking Regressor\")\n",
        "stack_reg_v3.fit(X, y)\n",
        "\n",
        "\n",
        "# --- 3. Prediction and Final Submission ---\n",
        "\n",
        "# Predict test set\n",
        "test_preds_stack_v3 = stack_reg_v3.predict(X_test)\n",
        "\n",
        "# Clip to 0 for submission safety\n",
        "final_preds_v3 = np.maximum(0, test_preds_stack_v3)\n",
        "\n",
        "# Save submission\n",
        "submission = test[['Item_Identifier','Outlet_Identifier']].copy()\n",
        "submission['Item_Outlet_Sales'] = final_preds_v3\n",
        "submission.to_csv(\"submission_final_stack.csv\", index=False)\n",
        "print(\"Saved submission_final_stack.csv\")"
      ]
    }
  ]
}